{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>lawData is an authenticated datasette (<code>0.64.1</code>) instance on fly.io deployed via litestream and docker, covering databases created from <code>/corpus-extractor</code> and <code>/corpus-x</code>. Revised flow uses Cloudflare R2 with credentials stored generally stored in <code>op://dev/lawdata</code>.</p>"},{"location":"#folder-structure","title":"Folder Structure","text":"Bash<pre><code>\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 queries # see metadata.yml referencing app/queries in relation to datasette-query-files\n\u2502   \u2502   \u251c\u2500\u2500 x # for x-based .sql queries\n\u2502   \u2502   \u251c\u2500\u2500 pdf # for pdf-based .sql queries\n\u2502   \u251c\u2500\u2500 plugins\n\u2502   \u251c\u2500\u2500 scripts\n\u2502   \u2502   \u251c\u2500\u2500 run.sh # called via Dockerfile CMD\n\u2502   \u251c\u2500\u2500 metadata.yml\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 main.py # contains the commands implemented in run.sh\n\u251c\u2500\u2500 data # see main.py in tandem with fly.toml which will look for / place .db files here\n\u251c\u2500\u2500 Dockerfile\n\n# while in the root folder, ensure that requirements.txt inside `app/` is updated via:\npoetry export -f requirements.txt -o app/requirements.txt --without-hashes\n</code></pre>"},{"location":"#docker-entrypoint","title":"Docker Entrypoint","text":"<p>The Dockerfile does not change directories via <code>WORKDIR</code>.</p> <p>This is because of <code>run.sh</code> which references 2 top-level directories:</p> Directory Description <code>/app</code> Initialized with <code>fly apps create</code> <code>/data</code> Separate volume created via <code>fly volumes create</code> and then attached to the app created via <code>fly apps create</code>. scripts/run.sh<pre><code>litestream restore -config etc/litestream-prod.yaml -v ${DB_FILE}\ndatasette serve --immutable ${DB_FILE} \\ # see data folder\n--host 0.0.0.0 \\\n--port 8080 \\ # same port in Dockerfile\n--metadata app/metadata.yml \\ # see app folder\n--plugins-dir app/plugins \\ # see app folder\n--setting default_cache_ttl 86400 \\\n--setting sql_time_limit_ms 20000 \\\n--setting allow_download off \\\n--cors\n</code></pre>"},{"location":"#assumptions","title":"Assumptions","text":"<ol> <li>sqlite databases are existing in r2 with credentials previously setup</li> <li>valid Dockerfile in root directory and the proper versions of the prerequisite apps are configured:<ol> <li><code>python</code>, 3.11</li> <li><code>litestream</code>, 0.39</li> <li><code>sqlite</code> 3.42</li> </ol> </li> <li>An updated <code>/app/requirements.txt</code> file is generated that will be used by the Dockerfile</li> </ol>"},{"location":"config/","title":"Datasette Plugins","text":""},{"location":"config/#datasette-auth-token","title":"<code>datasette-auth-token</code>","text":"Bash<pre><code>\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 metadata.yml # see datasette-auth-tokens referrencing `LAWSQL_BOT_TOKEN`\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 1.db\n\u2502   \u251c\u2500\u2500 2.db\n</code></pre> <p>The restored database/s can be accessed in a localhost container through a <code>datasette</code> instance via an authorization bearer token <code>LAWSQL_BOT_TOKEN</code>, if the same is enabled in the<code>metadata.yml</code> file.</p>"},{"location":"config/#configure-metadata","title":"Configure metadata","text":"<p>Edit the metadata.yml to allow access to the database without a token by commenting out:</p> YAML<pre><code>databases:\n# allow: --&gt; commented out\n#  bot_id: \"lex-bot\" --&gt; commented out\n</code></pre>"},{"location":"config/#access-local-venv-via-datasette","title":"Access local .venv via <code>datasette</code>","text":"Bash<pre><code># .env\ndatasette serve --immutable data/pdf.db data/x.db ...\n</code></pre>"},{"location":"config/#access-running-docker-container-via-docker-run","title":"Access running Docker container via <code>docker run</code>","text":"<p>Can create the docker image with:</p> Bash<pre><code>docker build -t lawdata . # Will look for Dockerfile inside the . folder\n</code></pre> <p>This will start the build process. If successful, the docker image will be built and appear in the list of Docker Images found in VS Code's Docker extension.</p> <p>Run the docker image locally with:</p> Bash<pre><code>docker run --publish 8080:8080 --env-file .env lawdata\n</code></pre>"},{"location":"config/#restore-via-runsh","title":"Restore via run.sh","text":"<p>The Dockerfile terminates with <code>run.sh</code>.</p> <p>Since, on initialization, the sqlite database file doesn't exist yet, it will use litestream's <code>restore</code> command to copy the AWS variant to a local container.</p> Bash Session<pre><code>No database found, restoring from replica if exists\n2022/12/10 05:42:11.906021 s3: restoring snapshot xxx/00000000 to /db/x.db.tmp\n2022/12/10 05:43:45.299375 s3: restoring wal files: generation=xxx index=[00000000,00000000]\n2022/12/10 05:43:45.494176 s3: downloaded wal xxx/00000000 elapsed=191.820083ms\n2022/12/10 05:43:45.566760 s3: applied wal xxx/00000000 elapsed=73.156208ms\n2022/12/10 05:43:45.566865 s3: renaming database from temporary location\nINFO:     Started server process [15]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n</code></pre>"},{"location":"config/#test-access-on-a-running-container","title":"Test access on a running container","text":"<p>Unauthorized:</p> Bash<pre><code>curl -IX get localhost:8080/x.json\n</code></pre> <p>Produces a HTTP/1.1  403 (FORBIDDEN) http status code:</p> Bash<pre><code>HTTP/1.1 403 Forbidden\ndate: x x x\nserver: uvicorn\ncontent-type: text/html; charset=utf-8\nTransfer-Encoding: chunked\n</code></pre> <p>Authorized:</p> <p>With xxx as <code>LAWSQL_BOT_TOKEN</code>, this results in a list of tables from restored the <code>x.db</code> via datasette + litestream:</p> Bash<pre><code>export token=&lt;whatever-value-of-LAWSQL_BOT_TOKEN&gt;\ncurl -H 'Authorization: Bearer ${token}' localhost:8080/x.json | jq\n</code></pre>"},{"location":"config/#datasette-auth-github","title":"<code>datasette-auth-github</code>","text":"Bash<pre><code>\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 metadata.yml # see datasette-auth-github referrencing `DATASETTE_GITHUB_AUTH_CLIENT_ID` and `DATASETTE_GITHUB_AUTH_CLIENT_SECRET`\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 1.db\n\u2502   \u251c\u2500\u2500 2.db\n</code></pre> <p>See <code>metadata.yaml</code>:</p> YAML<pre><code>databases:\nallow:\ngh_login:\n- justmars # allowed actor with datasette-auth-github\n</code></pre>"},{"location":"config/#datasette-query-files","title":"<code>datasette-query-files</code>","text":"Bash<pre><code>\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 queries # see metadata.yml referencing app/queries in relation to datasette-query-files\n\u2502   \u2502   \u251c\u2500\u2500 x # for x-based .sql queries\n\u2502   \u2502   \u251c\u2500\u2500 pdf # for pdf-based .sql queries\n</code></pre> <p>Unlike a default Datasette instance, canned SQL queries will not be found in the <code>metadata.yml</code>.</p> <p>The <code>datasette-query-files</code> plugin allows us to use a separate folder <code>/queries/</code> where each pairing of <code>.sql</code> and <code>.yml</code> becomes its own canned API endpoint.</p>"},{"location":"config/#native-sql-files","title":"Native .sql files","text":"<p>This setup makes it easier to write .sql files in VSCode with extensions (dbt formatter dependent on vscode-dbt):</p> JSON<pre><code>// settings.json\n\"files.associations\": {\n\"**/*.sql\": \"jinja-sql\",\n}\n</code></pre>"},{"location":"config/#relatively-complex-sql","title":"Relatively Complex SQL","text":"<p>The sqlite expressions are complex, making use of JSON1 and FTS5 extensions.</p>"},{"location":"config/#start-and-end-rows-precursor-to-fts5-snippet","title":"Start and End Rows, precursor to FTS5 snippet","text":"<p>Most of the queries utilize the following common table expression style:</p> SQL<pre><code>SELECT\nROW_NUMBER() over (ORDER BY cx.id) rn,\ncx.id row_idx,\nCOUNT(*) over () max_count\nFROM\nlex_tbl_codification_fts_units cx\nJOIN lex_tbl_codification_fts_units_fts\nON cx.rowid = lex_tbl_codification_fts_units_fts.rowid\nWHERE\ncx.codification_id = :code_id\nAND lex_tbl_codification_fts_units_fts match escape_fts(:q) -- escape_fts is a datasette-defined user function\n</code></pre> <p>This is an example of fetching the applicable rows for a given <code>code_id</code> with a matching full-text-search (fts) done on the <code>lex_tbl_codification_fts_units</code> table.</p> <p>It creates rows with the following fields under a designated order:</p> <ol> <li>row numbers <code>rn</code> for each matching row</li> <li>paired unique id <code>row_idx</code> corresponding to the <code>rn</code></li> <li>total number of rows <code>max_count</code></li> </ol> <p>Using this first CTE as the baseline, a second CTE will be used to filter the first CTE based on a <code>start_row</code> and an <code>end_row</code></p> SQL<pre><code>SELECT\nrn, row_idx, max_count\nFROM\nrowids_match_q\nWHERE\nrn BETWEEN CAST(:start AS INTEGER)\nAND CAST(:end AS INTEGER)\n</code></pre> <p>The reason for these preliminary CTEs is to limit the  rows that sqlite's fts5 snippet function will be called to operate on; if the snippet function were called in the first CTE, then all of the matching rows would have a computed value vs. the ranged rows limited by the <code>start</code> and <code>end</code> parameters.</p> SQL<pre><code>SELECT\nsnippet(\nlex_tbl_codification_fts_units_fts,\n0,\n'&lt;mark&gt;',\n'&lt;/mark&gt;',\n'...',\n15\n) matched_text\nFROM\nlex_tbl_codification_fts_units cx3\nJOIN lex_tbl_codification_fts_units_fts\nON lex_tbl_codification_fts_units_fts.rowid = cx3.rowid\nWHERE\ncx3.id = cx2.id -- cx2 is declared in the main SQL statement and will be based on the prefiltered rows\nAND lex_tbl_codification_fts_units_fts match escape_fts(:q)\n</code></pre> <p>The full SQL expression for this particular example can be found in <code>/queries/x/code_mp_fts_id.sql</code>.</p>"},{"location":"production/","title":"Fly.io Notes","text":""},{"location":"production/#initialization","title":"Initialization","text":"<p>Debug:</p> Bash<pre><code>fly doctor\nfly agent restart\n</code></pre>"},{"location":"production/#app-creation","title":"App creation","text":"<p>Check existing names then create the fly app with a unique name, we'll use as <code>lawdata</code>.</p> Bash<pre><code>fly apps create lawdata\n</code></pre>"},{"location":"production/#volume-creation","title":"Volume creation","text":"<p>After creating the app, create a volume that will be used by the app for persistent storage.</p> <p>Note that the app is separate from the volume.</p> <p>In case it already exists, it might be necessary to destroy the volume first and recreate it in case it has no more space.</p> Bash<pre><code>fly vol list\n# will display list of vol ids\nfly vol destroy &lt;instance-id&gt;\n# ? Are you sure you want to destroy this volume? Yes\n# Destroyed volume xxx from lawdata -- recall lawdata is the app name\n</code></pre> <p>We'll use the <code>db_lawdata</code> as the volume name to create, allocating 7GB as the volume size in the Singapore region with:</p> Bash<pre><code>fly vol create db_lawdata --region sin --size 7\nfly vol list\n</code></pre> <p>Review <code>fly.toml</code>:</p> <ol> <li><code>source</code> db_lawdata is created via <code>fly vol create db_lawdata</code>;</li> <li>the <code>destination</code> /data is prospective location of sqlite db; this is a folder of the volume that is described by the Dockerfile:</li> </ol> TOML<pre><code># fly.toml\n[mounts]\nsource = \"db_lawdata\"\ndestination = \"/data\"\n</code></pre> <p>See the associated Dockerfile:</p> Docker<pre><code># Dockerfile\nENV DB_FILE=/data/x.db\n</code></pre> <p>After the app is deployed, can ascertain the folder via</p> Bash<pre><code>fly ssh console\n# cd data\n# ls\n</code></pre> <p>The database file referred to is built with corpus-x.</p>"},{"location":"production/#setup-config","title":"Setup config","text":"<p>Review root <code>fly.toml</code>, specifically <code>app_name</code>, <code>mount.source</code>, and <code>services.internal_port</code></p> TOML<pre><code>app = \"lawdata\" # this was the name set during creation of the app\n[env]\nFLY_PRIMARY_REGION = \"sin\" # this is the region set during creation of the app's volume\n[mounts]\nsource = \"db_lawdata\" # this was the name set during creation of the app's volume; can verify this with fly volumes list\ndestination = \"/data\" # this is the folder to be created in the app for persistent storage, used in the Dockerfile\n[[services]]\ninternal_port = 8080 # will be used in the Dockerfile\n</code></pre>"},{"location":"production/#add-secrets","title":"Add secrets","text":"<p>Set the environment variables of the app:</p> Bash<pre><code>fly --app lawdata secrets import &lt; .env --stage\n</code></pre>"},{"location":"production/#set-environment-vars","title":"Set environment vars","text":"<p>See <code>.env.example</code> which outlines 5 variables that serve the following purposes</p> vars purpose <code>LITESTREAM_ACCESS_KEY_ID</code> &amp; <code>LITESTREAM_SECRET_ACCESS_KEY</code> aws credentials for litestream.io to restore a replica of previously saved &amp; replicated database to the volume created <code>LAWSQL_BOT_TOKEN</code> Bearer Token, a user-made credential for datasette (see datasette-auth-tokens plugin), to query the database; see allow list in <code>metadata.yml</code> <code>DATASETTE_GITHUB_AUTH_CLIENT_ID</code> &amp; <code>DATASETTE_GITHUB_AUTH_CLIENT_SECRET</code> github credentials, see (datasette-auth-github plugin), to login and access datasette via the production url, set the callback url in <code>Github's / Developer Settings /</code> oAuth Apps"},{"location":"production/#build-local-image-then-deploy-to-fly","title":"build local image then deploy to fly","text":"<p>Use the local Dockerfile to build the image before deploying the same to fly.io</p> Bash<pre><code>fly deploy --local-only\n</code></pre> <p>This will result in the following lines:</p> Bash Session<pre><code>==&gt; Verifying app config\n--&gt; Verified app config\n==&gt; Building image\n==&gt; Creating build context\n--&gt; Creating build context done\n==&gt; Building image with Docker\n--&gt; docker host: 20.10.21 linux aarch64\n...\n</code></pre> <p>Substantive steps of the Dockerfile to create the Docker image</p> Step Description Time (seconds) 2 Building python slim with relevant libs ~120 5 Building sqlite with extensions ~420 7 Install requirements.txt ~60 <p>After the image is built, this will push the image to fly:</p> Bash Session<pre><code>--&gt; Building image done\n==&gt; Pushing image to fly\nThe push refers to repository [registry.fly.io/corpus-x]\n...\ndeployment-aaa: xxx\n--&gt; Pushing image done\nimage: registry.fly.io/corpus-x:deployment-aaa\nimage size: 323 MB\n==&gt; Creating release\n--&gt; release v2 created\n--&gt; You can detach the terminal anytime without stopping the deployment\n</code></pre>"},{"location":"production/#add-certificate-to-production-url","title":"Add certificate to production url","text":"<p>When the app is first created, the following URL will be usable: <code>lawdata.fly.dev</code></p> <p>After a certificate is issued, this can become <code>lawdata.xyz</code>.</p> Bash<pre><code>fly ips list\nfly certs create lawdata.xyz\nfly certs create www.lawdata.xyz\n</code></pre> <p>Visit the fly.io dashboard and copy the <code>A</code> and <code>AAAA</code> values. These are also the same values reflected in <code>fly ips list</code>.</p> <p>After running <code>fly certs create www.lawdata.xyz</code>, the following prompt is displayed:</p> Bash Session<pre><code>You can configure your DNS for www.lawdata.xyz by:\n1: Adding an CNAME record to your DNS service which reads:\n    CNAME www. lawdata.fly.dev\n</code></pre>"},{"location":"production/#test-access-on-deployed-app","title":"Test access on deployed app","text":""},{"location":"production/#unauthorized","title":"Unauthorized","text":"<p>Without token:</p> Bash<pre><code>curl -IX get https://lawdata.fly.dev/x\n</code></pre> <p>Produces a HTTP/2 403 (FORBIDDEN) http status code:</p> Bash<pre><code>HTTP/2 403\ndate: x x x x\nserver: Fly/x x x\ncontent-type: text/html; charset=utf-8\nvia: 2 fly.io\nfly-request-id: x x x-sin\n</code></pre>"},{"location":"production/#authorized","title":"Authorized","text":"<p>With the url set at: <code>lawdata.fly.dev</code>, the database file at <code>x.db</code>, and the secret previously set for <code>LAWSQL_BOT_TOKEN</code>, can test a json list of tables with:</p> Bash<pre><code>export token=&lt;whatever-value-of-LAWSQL_BOT_TOKEN&gt;\ncurl -H \"Authorization: Bearer ${token}\" https://corpus-x.fly.dev/x.json | jq\n</code></pre>"}]}